---
title: 面试记录：KVCache、vLLM拟解决的问题
date: 2025-09-03 11:03:50
categories:
  - interview
tags:
  - LLM
  - interview
mathjax: true
---

# 前言

这道题目相对来说简单一些，考验的是`KVCache`和`vLLM`的实现原理。

<!-- more-->

# KVCache

在一些博客中，我们能看到，`KVCache`有一大堆的计算流程。这里也就不再重复说明了，本质上来说，就是证明了：

生成式`AI`中，下一个`token`的生成仅依赖上一个`token`信息。这也就意味着，计算下一个`token`的时候，非常依赖于上一个`token`的计算结果，尤其是其中的`Q`和`K`。

而当有了`KVCache`之后，之前计算过程中保存的`Q`和`K`就可以直接从缓存中获取，从而大大节省了计算量。也就是说，本质上还是用缓存减少计算量的特点，也就是通过空间换时间，用命中率代替计算量。

但在这里有一点需要明确的是，这一项确实有命中率的概念，但是命中率并不是最终的目标，因为核心目标只是希望预测下一个`token`的时候减少计算量，命中率是最终的现象。

# vLLM

虽然说这个计算没有说很复杂，但由于计算量极大，因此需要`vLLM`的`PageAttention`提供加速支持。

那么，`vLLM`又是做了什么事情呢？

首先，`vLLM`将所有的`KVCache`都分割为固定的块，每个块占据显存的一个部分，然后使用虚拟内存将离散存储的物理块映射到连续存储的逻辑块。

如果你有计算机基础的话，你会发现，这种管理方式与计算机原理中的分页表有着相似之处，只是管理的项并不是内存，而是显存。

所以，很明显，一个线程原本应当占据一整块存储空间，因为分页表机制，这个整块的存储空间被分割到多个物理存储空间中，但是又从逻辑上拼接为一个连续的存储空间。一方面，线程所需存储空间的分配更为灵活。同时，更为灵活的存储空间分配，减少了存储空间零散空余的问题，大幅提升了存储空间的利用率。

于是，回到`vLLM`，他所解决的问题也就非常明显：批处理计算过程中的显存碎片问题。

这里更需要注意一点，显存碎片问题与命中率基本上没有什么关系，所以不能往命中率上思考。