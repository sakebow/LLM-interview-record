---
title: 面试记录：大模型参数有哪些，什么场景下如何配置？
date: 2025-09-10 14:12:50
categories:
  - interview
tags:
  - LLM
  - interview
mathjax: true
---

# 前言

这个是相当考察经验的一道题。

<!-- more -->

# 参数有哪些

无论是使用`v1.chat.completions`，或者是`ChatOpenAI`，我们都会给定这么些参数：

| 参数 | 作用 | 调高的影响 | 调低/置零的影响 | 何时用 |
|:---:|:---:|:---:|:---:|:---:|
|`temperature`| 对下一个 token 概率分布做软化/放大 | 更发散、创意↑、事实漂移↑ | 更确定、重复↑、保守 | 创意写作↑；结构化输出↓ |
|`top-p`| 只在累计概率 ≤ p 的候选里采样 | 砍掉尾部噪声，稳定性↑ | 接近贪心，易重复 | 大多数采样任务的主力 |
|`top-k`| 只在概率前 k 名中采样 | 控制候选池大小、可抑制幻觉 | 接近贪心 | 和 top-p 二择一，少同时大值 |
|`typical_p`| “典型性”截断，保留信息量接近平均的 token | 更自然、少奇怪词 | 变保守 | 替代 top-p 的一种选择 |
|`repetition_penalty` / `frequency_penalty` | 惩罚已出现的 n-gram/词频 | 重复↓、有时语义漂移 | 重复↑ | 长文、对话避免啰嗦 |
| `presence_penalty` | 惩罚是否出现过（不看次数） | 主题探索↑、离题风险↑ | 主题集中 | 需要多样性/头脑风暴 |
| `no_repeat_ngram_size` | 禁止重复的 n-gram | 强力去重 | 可能卡壳 | 摘要/新闻式文本 |
| `max_new_tokens` / `min_new_tokens` | 限制输出长度 | 长度更稳定 | 可能截断/过短 | 需要定长/模板 |
| `stop` / `eos_token_ids` | 提前停止 | 防跑飞 | 可能早停 | 结构化/函数调用/JSON |
| `num_beams` | 同时扩展多路径取最优 | 一致性↑、多样性↓、慢 | 贪心 | 翻译/摘要等确定性任务 |
| `length_penalty` | 惩罚过短/过长 | 控制文风长度 | 可能啰嗦/干瘪 | 摘要/翻译 |

看上去很大一段，但是没关系，我们慢慢熟悉。

# 信息抽取、结构化JSON

比如说，用户提到了要去`cornhub`找些剥苞米的教程看看，就相当于要准确地提取到`cornhub`和`剥苞米`这两个关键词。也就是说，关键的是**确定性**和**合法率**。于是呢，经验上来说可以判定：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.0\sim0.2$
`top-p`|$0.9$
`repetition_penalty`|$1.05\sim1.1$
`do_sample`|`False`

# RAG

检索生成的过程其实类似信息抽取，其中给定的知识也是相对来说确定的，尽可能地减少大模型的发散也是相当重要的事情。因此推荐：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.1\sim0.3$
`top-p`|$0.9$
`repetition_penalty`|$1.05$

# 摘要

摘要虽然说基于给定的文本，但是允许在文本之上进一步总结归纳，需要一定的发散，因此在摘要场景下，相对来说需要更高的灵活性：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.3\sim0.7$
`top-p`|$0.9\sim0.95$
`length_penalty`|$1.0\sim1.2$

# 翻译

印象最深的自然是`My Little Pony`的漫画翻译了，那只喜欢押着韵说话的斑马，有些翻译大佬能够翻译地很押韵。当然，从这也就能看出来，翻译本质上也是某种意义上的创作。只是，大模型的发散并不能够控制发散方向，所以在某种程度上还是需要均衡考虑。一般的，以一致性为最优先，推荐如下配置：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.2\sim0.3$
`top-p`|$0.9\sim0.95$

而如果需要发散一些，显得更加跳脱一些，推荐配置：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.3\sim0.5$
`top-p`|$0.9\sim0.95$

# 创意写作

所谓创意，那便可以天马行空，让作品更有冲击感。当然，有些时候创飞了观众也是有可能的，只不过艺术角度看的话，创飞观众或许也不是坏事。

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.7\sim1.0$
`top-p`|$0.9\sim0.95$
`presence_penalty`|$0.3\sim0.8$

# 代码生成

代码文件相对来说也有严谨性和一致性，包括语义约束、语法约束等等。所以相对来说，比起发散性，更需要后评估。推荐配置：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.0\sim0.3$
`top-p`|$0.9\sim0.95$
`repetition_penalty`|$1.05$
`n`|$5$

也就是说，尽可能一次性给出几种结果，然后启用后评估检查可靠性，最后给出答案。

# CoT

思维链如果只需要一个结果的话，还是以稳定性为最优先，因此推荐：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.0\sim0.3$
`top-p`|$0.9\sim0.95$

如果需要多个结果，并在最终启用后评估表决最优结果，本质上也是一种创作（头脑风暴），因此需要更高的发散：

| 参数 | 建议值 |
|:---:|:---:|
`temperature`|$0.7$
`top-p`|$0.95$
`n`|$10\sim20$

# 调参顺序

那么讲了这么多，实际执行的时候我们应该做什么呢？

## 一个好的习惯是一个好的开始

首先当然是养成控制变量的习惯。我们完全可以随机定义变量，毕竟深度学习也是从随机参数开始学习。但是，后面调参的过程中，我们必须要保证每次只动一个参数，以保证我们能够准确地看出每个参数的实际效果。

## 从发散性开始

其次呢，我们最优先调整的，应当是`temperature`，毕竟发散性可以很明显地观察到，也可以反映在各种指标上。如果回答出现了**啰嗦**、**跑题**，应当立即降低。

当然，谈到`temperature`，就不得不考虑另外一个场景，就是`JSON`提取。如果在这种场景下，给出的`JSON`信息格式不对，或者出现不匹配的情况，需要额外配置`stop`参数。

## 然后是控制自我回声

其次调整的就是`repetition_penalty`或者`frequency_penalty`。多次尝试下，可以很明显地观察到回答在不停重复某些片段。如果**重复**和**自我回声**反复出现，则应当立即提高。

## 观察答案停止位置

停止位置除了`stop`之外，还有一个关键因素，就是`max_new_tokens`和`length_penalty`。当某些回答突然在一个地方中断的时候，就得继续调高了。

## 微调创意

除了`temperature`可以提供发散性，还有`presence_penalty`可以提供创意。通过提高`presence_penalty`，可以得到更加具有创意的答案。