---
title: 面试记录：请谈谈一下你对大模型的看法
date: 2025-08-22 10:00:33
categories:
  - interview
tags:
  - LLM
  - interview
---

# 前言

也许你一开始就在期待我开始讲什么`self-attention`。我承认，这已经算是必背科目，甚至算客观题了。

但是，从这个地方开始总归是有点追逐太阳的感觉，越追越累，还追不上。

所以，我们直接从太阳开始出发。

<!-- more -->

# 工作中遇到的大模型

目前因为各种各样的原因，接触到了昇腾平台，这个平台号称对大模型有着比较不错的支持。所以，我也尝试了一下在上面部署`DeepSeek`、`Qwen`以及各种闭源自研的央企大模型。

部署过程中，需要配置的包括`tokenizer.json`、`tokenizer_config.json`以及`mindie_config.json`，然后再去魔搭找到一个适配的模型，用`mindieservice_daemon`启动，就好了。默认配好的是`localhost:8090`端口，对外只需要控制`docker`映射即可。

这也算是运维角度的一个看法。

我们再深挖一下。

# 各种配置文件

## mindie_config.json

比如说，`mindie_config.json`，这个能够配置`MindIE`中的一些配置，包括有模型名称（`DeepSeek-R1-Dstill-Llama-70B`）、显卡数量（$4$）、显卡设备编号（`[0, 1, 2, 3]`）等等。有了这些，`MindIE`就会创建`HTTP`服务，共发布$4$个接口，包括`/generate`、`generate_stream`、`/v1/models`、`/v1/chat/completions`。

其中，`/generate`和`generate_stream`使用较少，也不会检查`model_name`是否在`mindie_config.json`中存在；而`/v1/models`则会列出所有`mindie_config.json`中配置的模型名称。最后呢，也就是我们的常客，`/v1/chat/completions`。他会检查`model_name`是否在`mindie_config.json`中存在，而且会提供兼容`OpenAI`的接口。

## tokenizer.json

`tokenizer.json`提供了完整序列化的`Fast`分词器，里面包含分词算法、词表、合并表、`normalization`、`preprocessor`、`postprocessor`等所有细节。

我们也经常能够在`Huggingface`、`modelscope`等网站中找到大模型参数的说明文件，其中说明文件也经常能够看到首先使用`AutoTokenizer`加载模型文件。就像这样：

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("/path/to/model")
```

首先呢，对于这个`/path/to/model`，我们首先检查是否存在`config.json`文件。依照这个文件，可以选出具体的分词器，这也是为什么模型文件中一旦缺少`config.json`文件就会报错的原因。

其次，在获取到分词器之后，还有一步，就是找到`tokenizer.json`文件进行反序列化，利用`PreTrainedTokenizerFast`构成`tokenizer.Tokenizer`对象。就像这样：

```python
from transformers import PreTrainedTokenizerFast
tok = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")
```

这个对象最终就可以开始进行大模型的下一步了，也就是输出文段编号。就像这样：

```python
out = tok(["some text", "more text"], return_tensors="pt", padding=True, truncation=True)
```

整个流程就是`Normalizer`->`PreTokenizer`->`Model`->`Post-Processor`->`Output`，从左到右功能依次是：

- `Normalizer`：大小写、Unicode归一化等；
- `PreTokenizer`：空白、字节级拆分等；
- `Model`：词表、合并表、概率等；
- `Post-Processor`：`BOS`、`EOS`、`PAD`、`UNK`等其他特殊分割、偏移对齐等；

后面可能还有一个`decoder`。

> 注：这里的`Model`指的是分词器的`model`，比如`BERT`的`WordPiece`模型。

## tokenizer_config.json

除了`tokenizer.Tokenizer`对象，我们还需要一个`tokenizer_config.json`文件。它相当于是`tokenizer.json`的一个补充，里面记载了一些特殊分词器的配置，包括换行、结束等等。如果没有的话，大模型生成这些也就无法识别，更没办法进行所需要的特殊处理。

到这里，就是高级一点的运维了。

# MindIE特性

当然，运维不光是运行部署，还有维护。显存爆了怎么办，并发不够怎么办之类的。我们也就可以从`MindIE`开始。

比如说，我们可以开启`CPU`的`KV Cache`缓存，帮助`GPU`减少`KV Cache`负担。

又比如说，我们可以修改`npuMemSize`，限制模型和`KV Cache`的显存占用，这样就会多一点点给到运行的并发。

如果显存不够的话，最大的影响就是大模型服务每秒的吞吐量，如果说极限吞吐量也只能达到`16 tokens/s`的话，这也就意味着最多最多也只有`10字/秒`的吞吐量，不优化将大幅降低用户体验。