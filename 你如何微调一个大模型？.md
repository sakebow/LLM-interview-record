---
title: 面试记录：你如何微调一个大模型？
date: 2025-09-18 09:18:00
categories:
  - interview
tags:
  - LLM
  - interview
mathjax: true
---

# 前言

这个问题问得相当大。大模型的微调分为好几个阶段，每个阶段都有自己的任务。接下来就有的讲了。

<!-- more -->

# BaseLine

首先，如果是拿到一个刚开发好的大模型，我们当然是首先确认好`BaseLine`。

如果你完全不明白，没事。我们回忆一下大模型怎么部署的。如果你彻底忘记了，看一下[这篇文章](/2025/08/22/interview/请谈谈一下你对大模型的看法/)，主要就是说，拿到参数之后，首先做的就是配置一大堆配置文件，然后用`AutoTokenizer`进行分词，然后用`AutoModelForCausalLM`进行加载。

那么其实问题就相对来说比较明确了。首先就是分词方案、分词器、词表等问题，都在配置文件中配好。

比如，我在确认当前大模型`CM`的时候，要求上下文是$57B$，然后分词方案采用常见的`SentencePiece`；其次，我们一般以因果语言建模作为训练目标，还有一些其他的任务目标可以额外配置，其他的任务有偏好优化与对齐等。

然后顺带确认一下上下文长度，这些可能影响到一些比较细节的配置，比如文本越长，`PPL`将会越低，但也可能在训练长度远小于推理长度而导致`PPL`抬头。

# 数据策划

完成`BaseLine`之后，就是数据策划了。这一步其实主要做的就是整理数据来源的组合与比例，然后做好去重和去泄露，过滤低质量的内容。

这个过程中，可以用信息熵、混合熵两个标准，如果熵值较低，则表示数据较单一。

除了熵值以外，还可以用参考`LM`测`PPL`值，若`PPL`高，则语料噪音过高；若`PPL`低，则语料重复内容较多，可能混入了较多的抄录与模板。

当然，考虑到部分场景还有`JSON`解析的需求，因此也有格式合法率指标。

# 预训练

准确的说，这一步就是一个`warming up`的过程。主要也是用`AdamW`练，顺带监控一下`PPL`，用`Loss`看看梯度的稳定性。

# 继续预训练

继续预训练，又称**领域自适应**，使用具体领域内的预料在训练一次。这个过程中主要也是看时间里的回答效果，监控一下召回率。

# 后训练

后训练就到了我们所熟知的`SFT`了。再就是用一些强化学习方法做好偏好优化。

# 评测

评测阶段需要观察`MMLU`（通识）、`MT-Bench`（多轮对话）、`HumanEval`（推理）、结构化合法率、安全等指标。

# 部署前优化

这个优化包含很多方面，包括量化（`W8A8`）、蒸馏（`Dstill`）、`KVCache`优化、`vLLM`或`MindSpore`优化，观察延迟、拒答、越狱等。

# 上线与持续学习

这些都做完了之后就需要上线了。上线之后，为了保证可持续性，一方面要持续收集用户给出的反馈内容，另一方面也需要冻结一部分权重，保证模型不会被用户所利用。